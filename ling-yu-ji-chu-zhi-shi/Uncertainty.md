# 不确定性

## 基本概念

### 认知不确定性 Epistemic/model uncertainty

由于训练数据是采样获取的，不能完全反应真实的数据分布，所以数据会带来噪声不确定性。具体表现为，使用同样的数据可能训练出不同权重的可解决相同问题的模型，换言之，模型对于数据可以有不同的认知，因此称为认知不确定性。

认知不确定性主要是由数据量少造成的，可以通过增大数据集来解决。

认知不确定性的建模主要通过在模型权重上放置先验分布建模，给定一些数据来捕捉权重的变化程度。可以分为两种具体的实现。

1. 网络分支进行学习
2. Dropout预测（多次测试计算方差）

### 噪声不确定性/偶然/任意/随机不确定性 Aleatoric uncertainty

数据内在的不确定性，收集更多数据也无法减少这种不确定性。噪声不确定性可以分为两类：

- homoscedastic同方差不确定性：对不同的输入保持不变 
- Heteroscedastic异方差不确定性：取决于模型的输入

噪声不确定性的建模主要通过在模型的输出上放置分布来建模。

## 应用

### 损失的重加权

#### GRAM（Gradient Rescaling Attention Model）

网络分支学习认知不确定性，得到attention mask，对梯度进行缩放

#### UDL（Uncertainty-Driven Loss）

认为不确定性高的像素更重要，需要加大关注，对其损失分配更大的权重。UDL比GRAM作用于梯度更精确性能更好。

### 图像降质的估计

1. paper1：模糊核估计
2. paper2：学习生成LR图像的退化过程的不确定性

### 数据增强

使用BN进行更快的不确定性计算；生成对抗样本